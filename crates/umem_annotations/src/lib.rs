use serde::{Deserialize, Serialize};
use std::sync::Arc;
use thiserror::Error;
use umem_core::{MemoryContent, MemoryKind, MemorySignals, Provenance};

use umem_ai::{
    GenerateObjectRequestBuilder, GenerateObjectRequestBuilderError,
    GenerateTextRequestBuilderError, LLMProvider, ResponseGeneratorError,
};

pub struct Annotation;

#[derive(Debug, Error)]
pub enum AnnotationError {
    #[error("llm response generate_text failed: {0}")]
    ResponseGeneratorError(#[from] ResponseGeneratorError),

    #[error("llm response generate_text failed: {0}")]
    GenerateTextRequestBuilderError(#[from] GenerateTextRequestBuilderError),

    #[error("llm response generate_object failed: {0}")]
    GenerateObjectRequestBuilderError(#[from] GenerateObjectRequestBuilderError),
}

const ANNOTATION_PROMPT: &str = r#"
You are a memory annotation system. Your task is to analyze raw content and extract structured memory metadata that can be stored and retrieved efficiently.
Given the input content, produce a JSON object with the following structure:
## content
### summary
Extract the key points from the content as a concise, information-dense summary. Requirements:
- Preserve specific details: names, dates, numbers, URLs, technical terms, and concrete values
- Focus on actionable information, facts, preferences, and decisions
- Omit filler words, pleasantries, and redundant context
- Use clear, direct language
- 1-4 sentences depending on content complexity
### tags
Extract 3-7 lowercase keywords that categorize and index this memory:
- Use singular forms (e.g., "project" not "projects")
- Include domain-specific terms, proper nouns (lowercased), and action verbs where relevant
- Prioritize terms useful for future retrieval
## kind
Classify the memory into exactly one type:
- **Semantic**: General knowledge, facts, concepts, definitions, explanations
- **Episodic**: Specific events, experiences, occurrences with temporal or spatial context
- **Procedural**: How-to knowledge, workflows, step-by-step processes, techniques, habits
- **Instruction**: Explicit directives, user preferences, rules, constraints, configurations
- **Relational**: Information about people, organizations, entities, and their relationships
- **Working**: Temporary context relevant only to an ongoing task or session
- **Prospective**: Future intentions, goals, plans, reminders, scheduled commitments
## signals
### certainty (0.0 - 1.0)
How confident you are in the accuracy and completeness of the extracted information:
- **0.9-1.0**: Explicitly stated facts, direct quotes, unambiguous instructions
- **0.6-0.8**: Reasonable inferences, implied information with strong context
- **0.3-0.5**: Uncertain interpretations, partial information, ambiguous statements
- **0.1-0.2**: Speculative, conflicting information, or low-quality source
### salience (0.0 - 1.0)
How important, actionable, or frequently needed this memory is likely to be:
- **0.9-1.0**: Critical preferences, key decisions, important instructions, identity-defining information
- **0.6-0.8**: Useful facts, notable events, relevant context for future tasks
- **0.3-0.5**: Background information, minor details, situational context
- **0.1-0.2**: Trivial details, transient information, low future utility
Note: certainty and salience cannot both be 0.0.
## provenance
### origin
Infer the source of the content:
- **User**: Content originates from a human user (input, message, document they provided)
- **Agent**: Content generated by an AI system, automated process, or tool output
### method
How this memory was derived:
- **Direct**: The content is being stored as-is or with minimal transformation
- **Extracted**: Information was extracted/summarized from larger content (include model name and original prompt if applicable)
- **Summarized**: Content was condensed from a longer source (include model name if applicable)
---
"#;

#[derive(Clone, schemars::JsonSchema, Serialize, Deserialize)]
pub struct LLMAnnotated {
    pub content: MemoryContent,
    pub kind: MemoryKind,
    pub signals: MemorySignals,
    pub provenance: Provenance,
}

impl Annotation {
    pub async fn generate(
        raw_content: impl Into<String>,
        provider: Arc<LLMProvider>,
        model_name: impl Into<String>,
        system_prompt: Option<String>,
    ) -> Result<LLMAnnotated, AnnotationError> {
        let request = GenerateObjectRequestBuilder::<LLMAnnotated>::new()
            .model(model_name)
            .system(system_prompt.unwrap_or(ANNOTATION_PROMPT.into()))
            .prompt(raw_content)
            .provider(Arc::clone(&provider))
            .max_output_tokens(10000)
            .temperature(0.7)
            .build()?;

        let annotations = umem_ai::generate_object(request).await?;
        Ok(annotations.output)
    }
}
